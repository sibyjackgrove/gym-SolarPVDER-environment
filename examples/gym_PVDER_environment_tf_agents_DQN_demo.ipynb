{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gym_PVDER_environment_tf-agents_DQN_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sibyjackgrove/gym-SolarPVDER-environment/blob/master/examples/gym_PVDER_environment_tf_agents_DQN_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zAlIYExjncYl"
      },
      "cell_type": "markdown",
      "source": [
        "## Clone gym-PVDER repository and install it"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wB3iRbqyxZfC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://sibyjackgrove:@github.com/sibyjackgrove/gym-SolarPVDER-environment.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PwijcY_93R1i"
      },
      "cell_type": "markdown",
      "source": [
        "## Go to directory and do pip install"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L2wwEa3szq-X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd gym-SolarPVDER-environment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WzRR3E2Hzlw4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VOtpJ-Vr0cmh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKLoDRGbbFX0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install tf-nightly and tf-agents"
      ]
    },
    {
      "metadata": {
        "id": "JyIdCvWTbEh1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-gpu\n",
        "!pip install tfp-nightly\n",
        "!pip install tf-agents-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GmIwxfE3n0Et"
      },
      "cell_type": "markdown",
      "source": [
        "## Import the necessary modules"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lcF_Aocz0i2h",
        "outputId": "e0f9ee90-d9f9-458f-d714-dbd9601e6837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_PVDER\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.agents.dqn import q_network\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import trajectory\n",
        "from tf_agents.metrics import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.drivers import dynamic_episode_driver,dynamic_step_driver\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.utils import common\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "print(tf.__version__)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.1-dev20190327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gUrV7xsna-mY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n"
      ]
    },
    {
      "metadata": {
        "id": "G7tQqjnFa-mZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env_name = 'PVDER-v0'  # @param\n",
        "num_iterations = 20000  # @param\n",
        "\n",
        "initial_collect_steps = 1000  # @param\n",
        "collect_steps_per_iteration = 1  # @param\n",
        "replay_buffer_capacity = 100000  # @param\n",
        "\n",
        "fc_layer_params = (100,)\n",
        "\n",
        "batch_size = 64  # @param\n",
        "learning_rate = 1e-3  # @param\n",
        "log_interval = 200  # @param\n",
        "\n",
        "num_eval_episodes = 10  # @param\n",
        "eval_interval = 1000  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NkeKGPhQa-mc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4-_Uiy_14QaH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = suite_gym.load(env_name) #Load environment using tf-agents environment loader for gym\n",
        "env.render()\n",
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tIqzX2R_a-mf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JYWHRiUjqjPr"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a DQN agent"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B7JuNNr533ze",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q_net = q_network.QNetwork(train_env.observation_spec(),\n",
        "                           train_env.action_spec(),\n",
        "                           fc_layer_params=fc_layer_params)\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.compat.v2.Variable(0)\n",
        "\n",
        "tf_agent = dqn_agent.DqnAgent(train_env.time_step_spec(),\n",
        "                              train_env.action_spec(),\n",
        "                              q_network=q_net,\n",
        "                              optimizer=optimizer,\n",
        "                              td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
        "                              train_step_counter=train_step_counter)\n",
        "tf_agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_4r4bssma-mm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create policies from the agent"
      ]
    },
    {
      "metadata": {
        "id": "F9ijKlDMa-mn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_policy = tf_agent.policy\n",
        "collect_policy = tf_agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(time_step_spec=train_env.time_step_spec(),\n",
        "                                                action_spec=train_env.action_spec())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rYcoV6Ga-mr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create tf-agents driver for computing average return using tf-agents metrics module"
      ]
    },
    {
      "metadata": {
        "id": "0fkeM_jOa-ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "average_return = tf_metrics.AverageReturnMetric()\n",
        "env_episodes = tf_metrics.NumberOfEpisodes()\n",
        "env_steps = tf_metrics.EnvironmentSteps()\n",
        "average_return_observer = [average_return, env_episodes, env_steps]\n",
        "\n",
        "def compute_average_return(num_episodes = num_eval_episodes):\n",
        "    average_return_driver = dynamic_episode_driver.DynamicEpisodeDriver(eval_env, eval_policy, average_return_observer, num_episodes=num_episodes)\n",
        "    # Initial driver.run will reset the environment and initialize the policy.\n",
        "    final_time_step, policy_state = average_return_driver.run()\n",
        "\n",
        "    print('final_time_step', final_time_step)\n",
        "    print('Number of Steps: ', env_steps.result().numpy())\n",
        "    print('Number of Episodes: ', env_episodes.result().numpy())\n",
        "    print('Average Return: ', average_return.result().numpy())\n",
        "    \n",
        "    return average_return.result().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pags9UYmjlMU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create replay Buffer"
      ]
    },
    {
      "metadata": {
        "id": "WiUKAGfdjlc7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(data_spec=tf_agent.collect_data_spec,\n",
        "                                                               batch_size=train_env.batch_size,\n",
        "                                                               max_length=replay_buffer_capacity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zQYIjbvGkFJa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create tf-agents driver for Data Collection"
      ]
    },
    {
      "metadata": {
        "id": "U-4mv3ThkJw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_collect_observer = [replay_buffer.add_batch,env_episodes,env_steps]\n",
        "\n",
        "def collect_data(num_steps,VERBOSE=False):\n",
        "    data_collect_driver = dynamic_step_driver.DynamicStepDriver(train_env, collect_policy, data_collect_observer, num_steps=num_steps)\n",
        "    # Initial driver.run will reset the environment and initialize the policy.\n",
        "    final_time_step, policy_state = data_collect_driver.run()\n",
        "    if VERBOSE:\n",
        "        print('final_time_step', final_time_step)\n",
        "        print('Number of Steps: ', env_steps.result().numpy())\n",
        "        print('Number of Episodes: ', env_episodes.result().numpy())\n",
        "\n",
        "collect_data(initial_collect_steps,VERBOSE=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nFHfgUOWo4pf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EpPGrXjotrih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the agent"
      ]
    },
    {
      "metadata": {
        "id": "4cOMoTBntu_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_average_return(num_episodes = num_eval_episodes)\n",
        "\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    \n",
        "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "    collect_data(collect_steps_per_iteration)\n",
        "    \n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = tf_agent.train(experience)\n",
        "\n",
        "    step = tf_agent.train_step_counter.numpy()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
        "    if step % eval_interval == 0:\n",
        "        print('Evaluating agent at step = {}'.format(step))\n",
        "        avg_return = compute_average_return(num_episodes = num_eval_episodes)\n",
        "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "        returns.append(avg_return)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "we33WPlzO93D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot average return"
      ]
    },
    {
      "metadata": {
        "id": "mRLD4Z-kNGIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dIhmmf7DOvFZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test the trained agent's policy for a few episodes"
      ]
    },
    {
      "metadata": {
        "id": "0Uh76SgoNnPU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_episodes = 3\n",
        "for _ in range(num_episodes):\n",
        "    time_step = eval_env.reset()\n",
        "    while not time_step.is_last():\n",
        "        action_step = tf_agent.policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        print('Action:{}'.format(action_step.action))\n",
        "        eval_py_env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}