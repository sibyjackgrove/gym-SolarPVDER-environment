{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sibyjackgrove/gym-SolarPVDER-environment/blob/master/examples/gym_PVDER_environment_import_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAlIYExjncYl"
   },
   "source": [
    "## Clone gym-PVDER repository and install it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wB3iRbqyxZfC"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/sibyjackgrove/gym-SolarPVDER-environment.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PwijcY_93R1i"
   },
   "source": [
    "## Go to directory and do pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "L2wwEa3szq-X"
   },
   "outputs": [],
   "source": [
    "cd gym-SolarPVDER-environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WzRR3E2Hzlw4"
   },
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VOtpJ-Vr0cmh"
   },
   "outputs": [],
   "source": [
    "pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmIwxfE3n0Et"
   },
   "source": [
    "## Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcF_Aocz0i2h"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_PVDER\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.dqn import q_network\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import trajectory\n",
    "from tf_agents.metrics import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_name = 'PVDER-v0'  # @param\n",
    "num_iterations = 20000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 100000  # @param\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64  # @param\n",
    "learning_rate = 1e-3  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 10  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-_Uiy_14QaH"
   },
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name) #Load environment using tf-agents environment loader for gym\n",
    "env.render()\n",
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYWHRiUjqjPr"
   },
   "source": [
    "## Create a DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7JuNNr533ze"
   },
   "outputs": [],
   "source": [
    "q_net = q_network.QNetwork(train_env.observation_spec(),\n",
    "                           train_env.action_spec(),\n",
    "                           fc_layer_params=fc_layer_params)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = dqn_agent.DqnAgent(train_env.time_step_spec(),\n",
    "                              train_env.action_spec(),\n",
    "                              q_network=q_net,\n",
    "                              optimizer=optimizer,\n",
    "                              td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
    "                              train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create policies from the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(time_step_spec=train_env.time_step_spec(),\n",
    "                                                action_spec=train_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate policy using tf-agents drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_return = tf_metrics.AverageReturnMetric()\n",
    "num_episodes = tf_metrics.NumberOfEpisodes()\n",
    "env_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [average_return, num_episodes, env_steps]\n",
    "\n",
    "driver = dynamic_episode_driver.DynamicEpisodeDriver(eval_env, random_policy, observers, num_episodes=2)\n",
    "\n",
    "# Initial driver.run will reset the environment and initialize the policy.\n",
    "final_time_step, policy_state = driver.run()\n",
    "\n",
    "print('final_time_step', final_time_step)\n",
    "print('Number of Steps: ', env_steps.result().numpy())\n",
    "print('Number of Episodes: ', num_episodes.result().numpy())\n",
    "print('Average Return: ', average_return.result())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "gym_PVDER_environment_import_test.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
